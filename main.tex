
\documentclass[aspectratio=169]{beamer}

\usepackage{comment}

%% Pkg's to use the inkscape snippet
\usepackage{import}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{hyperref}
\usepackage{xcolor}
%%
\title[Penrose Pseudoinverse]{Moore-Penrose Pseudoinverse}
\subtitle{Classical least squares problem}
\author{J. Antonio Ortega}
\institute{UNAM} 
\date{\today}
\logo{\includegraphics[width=10mm]{./figures/logoFI.pdf}  }

%% Definitions

\newcommand{\incfig}[2][1]{%
    \def\svgwidth{#1\columnwidth}
    \import{./figures/}{#2.pdf_tex} }

\pdfsuppresswarningpagegroup=1

%%


\begin{document}
% has to be loaded outside of a frame to work!

\frame{\titlepage}

\begin{frame} % 3 subsystems
	\frametitle{Motivational Example}  % {Motivational Example}
	\begin{columns}

	\column{0.49\linewidth}
	

	Consider the following equation:

	\begin{equation*}
		\underbrace{\begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 6 \end{bmatrix}}_A \begin{bmatrix} i \\ j  \end{bmatrix} = \begin{bmatrix} 2 \\ 3 \\ 8\end{bmatrix}
	\end{equation*}
	

	\column{0.49\linewidth}
	\begin{figure}[ht]
    		\centering
	    \incfig{satelite}
	\end{figure}
	\end{columns}


\end{frame}

\begin{frame} % 3 subsystems
	\frametitle{Motivational Example}  % {Motivational Example}
	\begin{columns}

	\column{0.49\linewidth}
	

	
It can be seen as tree different subsystems:

\begin{equation*}
	\Sigma_1 := \left\{  \begin{bmatrix}	1 & 1 \\ 1 & 2	\end{bmatrix} \begin{bmatrix}	i \\ j	\end{bmatrix} = \begin{bmatrix} 2 \\ 3 \end{bmatrix}\right. \implies \quad [i, \, j]^T = [1, \, 1]^T
\end{equation*}

\begin{equation*}
	\Sigma_2 := \left\{  \begin{bmatrix}	1 & 1 \\ 1 & 6	\end{bmatrix} \begin{bmatrix}	i \\ j	\end{bmatrix} = \begin{bmatrix} 2 \\ 8 \end{bmatrix}\right. \implies \quad [i, \, j]^T = [0.8 , \, 1.2 ]^T
\end{equation*}

\begin{equation*}
	\Sigma_3 := \left\{  \begin{bmatrix}	1 & 2 \\ 1 & 6	\end{bmatrix} \begin{bmatrix}	i \\ j	\end{bmatrix} = \begin{bmatrix} 3 \\ 8 \end{bmatrix}\right. \implies \quad [i, \, j]^T = [0.5, \, 0.25]^T
\end{equation*}

	\column{0.49\linewidth}
	\begin{figure}[ht]
    		\centering
	    \incfig{satelite}
	\end{figure}
	\end{columns}


\end{frame}

\begin{frame} % Satelite example
	\frametitle{Motivational Example}  % {Motivational Example}
	\begin{columns}

	\column{0.49\linewidth}

	Consider the following equation:

	\begin{equation*}
		\underbrace{\begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 6 \end{bmatrix}}_A \begin{bmatrix} i \\ j  \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \\ b_3\end{bmatrix}
	\end{equation*}

	\uncover<2->{\textcolor{violet}{Is there solution for this system?}}
	\vspace*{10mm}
	
	\uncover<3->{\textcolor{violet}{There will be solution iff $b=\left[ b_1 \, b_2 \, b_3\right]^T \in \mathcal{C}(A)$ \textit{column space} of $A$ i.e., $b$ is a linear combination of the columns of $A$} } 

	\column{0.49\linewidth}
	\begin{figure}[ht]
    		\centering
	    \incfig{satelite}
	    \label{fig:satelite}
	\end{figure}
	\end{columns}


\end{frame}



\begin{frame} % Problem statement
	\frametitle{How to solve it in the \textit{best possible way}?}
	
	\begin{columns}

	\column{0.49\linewidth}
	\begin{itemize}
		\item $Ax = b$ may have no solution
	\end{itemize}	

	Denote $a_1$ and $a_2$ as the columns of the matrix $A$, thus,

	\[ \mathcal{C}(A) := \{ b \in \mathbb{R}^3 | Ax = b, \, x \in \mathbb{R}^2 \} \]

	that is, all the linear combinations of the columns of $A$.

	There exists a unique vector on $\mathcal{C}(A)$ nearest to $b$

	\[ p = a_1 \hat{x}_1 + a_2 \hat{x}_2 = A\hat{x} \] 

	\noindent where $p$ is the projection of $b$ onto the column space!
\begin{itemize}
	\item A possible alternative $A \hat{x} = p$ 
\end{itemize}
	\column{0.49\linewidth}

\begin{figure}[ht]
    \centering
    \incfig{mainissue}
    \label{fig:mainissue}
\end{figure}

\end{columns}
\end{frame}
\begin{comment}



\begin{frame} % Motivational Example
	\frametitle{Motivational Example}
	
	\begin{columns}

	\column{0.49\linewidth}
		
	Consider the following linear system's equation:

	\begin{align*}
		i - k &= 3 \\
		i + j &= 5 \\
		2i + j + 5k &= 14
	\end{align*}

	\begin{equation*}
		\underbrace{\begin{bmatrix}
			1 & 0 & -1 \\ 1 & 1 & 0 \\ 2 & 1 & 5
		\end{bmatrix}}_{H_3} \underbrace{\begin{bmatrix} i \\ j \\ k \end{bmatrix}}_{{z}_\ast} =  \underbrace{\begin{bmatrix} 3 \\ 5 \\ 14 \end{bmatrix}}_{{x}_\ast}
	\end{equation*}

	\uncover<2->{\textcolor{violet}{If we have just two equations?}}

	\column{0.49\linewidth}
	
\begin{figure}[ht]
    \centering
    \incfig{transRankcomp}
    \label{fig:transRankcomp}
\end{figure}

	\end{columns}

\end{frame}

\begin{frame}  % Motivational Example
	\frametitle{Motivational Example}
\begin{columns}
\column{0.49\linewidth}

	\begin{equation*}
		\underbrace{\begin{bmatrix}
			1 & 0 & -1 \\ 1 & 1 & 0 
		\end{bmatrix}}_{H_2} \begin{bmatrix} i \\ j \\ k \end{bmatrix} =  \begin{bmatrix} 3 \\ 5 \end{bmatrix}
	\end{equation*}

	\textcolor{violet}{How many solutions they are?}

	\uncover<2->{
	\begin{align*}
	i-k &= 3,\\
	j+ i &= 5 
	\end{align*}	
	
	\begin{equation*}
		\mathcal{S}:= \left\{ \begin{matrix} k &= i -3 \\ j &= 5 -i \end{matrix} \right.
	\end{equation*}
	
	All $\mathcal{S}$ are solutions of the equation's system $\forall \, i$

	}

\column{0.49\linewidth}
\uncover<2->{
\begin{figure}[ht]
    \centering
    \incfig{sys2rank}
    \label{fig:sys2rank}
\end{figure}
}
\end{columns}

\end{frame}


\end{comment}

\begin{frame} % Decomposition

\begin{columns}

\column{0.49\linewidth}
		
{\color{violet} \textbf{Lemma}:} Let $x$ be a vector and $\mathcal{L}$ is a linear manifold in $\mathbb{R}^n$
(\textit{i.e., } if $p, \, q \in \mathcal{L}$, then $\alpha i + \beta j \in \mathcal{L}$ for any scalars $\alpha, \, \beta$).

Then if

\[ b = p + e \]

\noindent where $ p \in \mathcal{L}$ and $ e \perp \mathcal{L}$,
then $p$ is \textit{"nearest"} to $b$, or, in other words, it is the \textcolor{violet}{projection}
of $b$ to the \textcolor{violet}{manifold} $\mathcal{L}$.
		
\column{0.49\linewidth}

\begin{figure}[ht]
    \centering
    \incfig{decompositionvector}
    \label{fig:decompositionvector}
\end{figure}

\end{columns}
	
\end{frame}

\begin{frame}{} % Pitagoras
	
	\begin{columns}
	
	\column{0.49\linewidth}
	

{\color{violet} \textit{Proof.}:} For any $q \in \mathcal{L}$ we have

\begin{align*}
	\Vert b-q \Vert^2 &=  \Vert  p + e - q \Vert^2\\ 
	&= \Vert (p - q) + e \Vert^2 \\
	&= \Vert (p - q) \Vert^2  + 2(p-q)^Te+ \Vert e \Vert^2\\ 
	&= \Vert (p - q) \Vert^2 + \Vert e \Vert^2 \\
	& \geq \Vert e \Vert^2
\end{align*}
		
\column{0.49\linewidth}
		
\begin{figure}[ht]
    \centering
    \incfig{pitagoras}
    \label{fig:pitagoras}
\end{figure}

\end{columns}

\end{frame}

\begin{comment}

\begin{frame} % Lttle recall
	\frametitle{Little Recall}
	\framesubtitle{Linear mappings with non-sqare matrix}
	
\begin{figure}[ht]
    \centering
    \incfig{lineartrans}
    \label{fig:lineartrans}
\end{figure}

\end{frame}

\end{comment}

\begin{frame} % The Four Subspaces
	\frametitle{A little recall}{The four subspaces of linear algebra. Lecture of Dr. Strang \href{ https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-10-the-four-fundamental-subspaces/ }{[Link]}}
 
\begin{figure}[ht]
    \centering
    \incfig[0.9]{foursubspaces}
    \label{fig:foursubspaces}
\end{figure}

\end{frame}

\begin{frame}{} % minimizationTheorem
	
{\color{violet} \textbf{Theorem}:} Let $b$ be an $n-$dimensional real vector and $A \in \mathbb{R}^{n \times m}$.\\
	
\begin{itemize}
	\item \textcolor{violet}{1.} There is always a vector, in fact a unique vector $\hat{x}$ of minimal (Euclidian norm), which minimizes

		\[\hat x = argmin \left( \Vert b - A x \Vert^2 \right) . \]
	
	\item \textcolor{violet}{2.}The vector $\hat{x}$ is the unique vector in the range
		
	\[ \mathcal{R}(A^T) := \{ x : x = A^T b, b\in \mathbb{R}^n  \} \]
	
	\noindent which satisfies the equation
	
	\[ A \hat{x} = p \]
	
	\noindent where $p$ is the projection of $b$ on $\mathcal{R}(A) = \mathcal{C}(A).$

	\end{itemize}
\end{frame}

\begin{frame} % theoremPart1
	\begin{columns}
	
	\column{0.49\linewidth}

		\textcolor{violet}{1.} There is always a vector, in fact a unique vector $\hat{x}$ of minimal (Euclidian norm), which minimizes

		\[ \Vert b - A x \Vert^2. \]
		
	\column{0.49\linewidth}

\begin{figure}[ht]
    \centering
    \incfig{mainissue}
    \label{fig:minhatx}
\end{figure}

	\end{columns}
\end{frame}

\begin{frame} % theoremPart1
	\begin{columns}
	
	\column{0.49\linewidth}
	\begin{align*}
		e &\perp \mathcal{C(A)} \\
		b- p & \perp \mathcal{C(A)} \\
		b- A \hat{x} & \perp \mathcal{C(A)}
	\end{align*}
	
	\begin{equation*}
		\begin{bmatrix} a_1^T \\ a_2^T\end{bmatrix} \left(b - A \hat{x} \right) = 0
	\end{equation*}

	\begin{equation*}
		A^T \left(b - A \hat{x} \right) = 0
	\end{equation*}
	
	\begin{equation*}	
		A^T e = 0 \implies e \in \mathcal{N}(A^T)
	\end{equation*}	

	\begin{equation*}
		A^T A \hat{x} = A^T b
	\end{equation*}

	\column{0.49\linewidth}

\begin{figure}[ht]
    \centering
    \incfig{mainissue}
    \label{fig:minhatx}
\end{figure}

	\end{columns}
\end{frame}

\begin{frame}{}  % nullSpace of Z
	
	{\color{violet} \textit{Proof.}:} We can write
		\[ b = \hat{b} + \tilde{b} \]
		\noindent where $\hat{b}$ is the projection of $b$ on the kernel (\textit{null space})
		
		\[\mathcal{N}(A^T) := \{ b\in \mathbb{R}^n | A^T b = 0  \}. \]
		
		Since $Ax \in \mathcal{R}(A) $ for any $x \in \mathbb{R}^m$, it follows that
		
		\[ \hat{b} - Ax \in \mathcal{R}(A) \]
		
		\noindent and, since $\tilde{b} \in \mathcal{R^\perp}(A)$,
		
		\[ \tilde{b} \perp  \hat{b} - Ax, \, \therefore  \]
		

		\begin{align*}
		\Vert b- Ax \Vert^2 &= \Vert (\hat{b}- Ax) + \tilde{b} \Vert^2\\
		&=\Vert \hat{b}- Ax  \Vert^2 + \Vert \tilde{b} \Vert^2 \geq \Vert \tilde{b} \Vert^2 = \Vert b - \hat{b} \Vert^2
		\end{align*}
\end{frame}

\begin{frame}{}  % x_{star} minimization
	
	$\Vert b- Ax \Vert^2 \geq  \Vert b - \hat{b} \Vert^2$ This low bound is attainable since $\hat{b}$, being the range of $A$, is the afterimage of some $x^\ast$, that is, $\hat{b}= A x^\ast$.\\
	\vspace*{5mm}
	\textcolor{violet}{1.} Let us show that $x^\ast$ has a minimal norm.
	
	\[ x^\ast = \hat{x}^\ast + \tilde{x}^\ast \]
	
	\noindent where $\hat{x}^\ast \in \mathcal{R}(A^\perp)$ and $\tilde{x}^\ast \in \mathcal{N}(A)$.\\
	\vspace*{5mm}	
	Thus, $A x^\ast = A\hat{x}$ we have $\Vert b- Ax^\ast \Vert^2 = \Vert b- A \hat{x} \Vert^2$ and $\Vert x^\ast \Vert^2 \geq \Vert \hat{x}^\ast \Vert^2 $.\\
	\vspace*{5mm}
	{\color{violet} So, $x^\ast$ may be selected equal to $\hat{x}^\ast.$ }
\end{frame}

\begin{frame}{} % Unicity of the solution
	

	\textcolor{violet}{2.} Show that $x^\ast = \hat{x}^\ast$ is unique. Suppose that $A x^\ast = A x^{\ast\ast}= p$. Then
	\[ (x^\ast - x^{\ast\ast} ) \in \mathcal{R}(A^T) \]
	But, $A(x^\ast - x^{\ast\ast} ) = 0$, therefore $ (x^\ast - x^{\ast\ast} ) \in \mathcal{N}(A) = \mathcal{C^\perp}(A^\perp) $
	
	Thus, $(x^\ast - x^{\ast\ast} )$ is orthogonal to itself! i.e., $\Vert x^\ast  - x^{\ast\ast} \Vert^2 = 0 \implies x^\ast = x^{\ast\ast}. $
\begin{figure}[ht]
    \centering
    \incfig[0.8]{diagrowspace}
    \label{fig:diagrowspace}
\end{figure}
\end{frame}

\begin{comment}

\begin{frame}{}  %Corollary
	
	{\color{violet} \textbf{Corollary}:}  $\Vert b - A x \Vert^2$ is minimized by $x_0$ if and only if $A x_0 = \hat{b}$ where $\hat{b}$ is the projection of $b$ on $\mathcal{R}(A)$.
	\vspace*{5mm}
		
	{\color{violet} \textbf{Corollary}:} There is always an $n-$dimensional vector $y$ such that
	\[ \Vert b - A A^T y\Vert^2 = \inf_ x \Vert b - A x \Vert^2,\]
	\noindent and if
	\[ \Vert b - A x_0 \Vert^2 = \inf_ x \Vert b - A x \Vert^2,\]
	\noindent then
	\[ \Vert x_0 \Vert^2 \geq \Vert A^T y\Vert^2\]
	with strict inequality unless $x_0 = A^T y$. The vector y satisfies the equation $A A^T y = \hat{b}$
\end{frame}

\end{comment}
\begin{frame}{} % mainTheorem
	
	{\color{violet} \textbf{Theorem}:} Among those vectors $x$, which minimize $\Vert b - A x\Vert^2$, $\hat{x}$, the one having minimal norm, is the unique vector of the form:
	\[\hat{x}= A^Ty,\]
	\noindent satisfying
	\[ A^T A \hat{x} = A^T b. \]

Therefore, 

\[ 	\hat x = (A^T A )^{-1} A^T b.	\]

The Moore-Penrose Pseudoinverse is defined by

\[ (A^T A )^{-1} A^T \]\\

Ps. Try to proof that if A is \textit{full-column rank} the square matrix is invertible.

\end{frame}

\begin{frame}
	\frametitle{$A^T A$ is invertible if it's full-column rank }

	Consider that 
	\[  A^T A x =0.  \]
	
	Thus, the column space $\mathcal{C}(A)$ is in the null space of $A^T$. But we know that $ \mathcal{C}(A) \perp \mathcal{N}(A^T) $, which implies that $A x = 0$.



If $A$ is full-column rank, the columns are linearly independient, which implies that if $A x = 0 \implies x = 0$.\\

	Therefore $\mathcal{N}(A^T A) = \{ 0 \}$  which implies that $A^T A$ is invertible.

\end{frame}

\begin{frame} % Satelite example
	\frametitle{Motivational Example}  % {Motivational Example}
	\begin{columns}

	\column{0.49\linewidth}

	Consider the following equation:

	\begin{equation*}
		\underbrace{\begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 6 \end{bmatrix}}_A \begin{bmatrix} i \\ j  \end{bmatrix} = \begin{bmatrix} 2 \\ 3 \\ 8 \end{bmatrix}
	\end{equation*}

	We need to minimize $ \Vert Ax - b  \Vert^2$

	\[ \Vert Ax - b  \Vert^2 = (i + j -2)^2 + (i + 2j -3)^2 + (i + 6j -8)^2 \] 
		

	\column{0.49\linewidth}
	\begin{figure}[ht]
    		\centering
	    \incfig{satelite}
	\end{figure}
	\end{columns}

\end{frame}

\begin{frame}[t]
	\frametitle{Motivational Example}  % {Motivational Example}
	
	\[ \Vert Ax - b  \Vert^2 = (i + j -2)^2 + (i + 2j -3)^2 + (i + 6j -8)^2 \] 

	\begin{align*}
		\frac{\partial}{\partial i}(\Vert Ax - b  \Vert^2 ) &= 0 \implies 3i  + 9j = 13 \\
		\frac{\partial}{\partial j}(\Vert Ax - b  \Vert^2 ) &= 0 \implies 9i + 41 j = 56
	\end{align*}

\begin{columns}	

\column{0.49\linewidth}

\[ \Sigma_m := \left\{ \begin{matrix} 3i  + 9j = 13 \\ 9i + 41 j = 56 \end{matrix} \right. \]

\[  [i, \, j]_{\Sigma m} = [0.6905 , \, 1.2143] \]

\column{0.49\linewidth}

Do you remember the expression $ A^T A \hat x = A^T b $

\[ A^TA = \begin{bmatrix} 3 & 9 \\ 9 & 41 \end{bmatrix} \textnormal{ and } A^T b =  \begin{bmatrix} 13 & 56 \end{bmatrix} \] 

\end{columns}
\end{frame}


\begin{frame} % Satelite example
	\frametitle{Motivational Example}  % {Motivational Example}
	\begin{columns}

	\column{0.49\linewidth}

	Consider the following equation:

	\begin{equation*}
		\underbrace{\begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 6 \end{bmatrix}}_A \begin{bmatrix} i \\ j  \end{bmatrix} = \begin{bmatrix} 2 \\ 3 \\ 8 \end{bmatrix}
	\end{equation*}

So, the best possible solution of this system is:

\begin{align*}
	A x &= b \\
	A^+ A x &= A^+ b \\
	x &= A^+ b\\
	x & = [0.6905 \quad 1.2143]^T
\end{align*}
	\column{0.49\linewidth}
	\begin{figure}[ht]
    		\centering
	    \incfig{satelite}
	\end{figure}
	\end{columns}

\end{frame}

\begin{frame}
	\frametitle{Example}
\begin{columns}

\column{0.49\linewidth}

	Consider a classical least square problem, find the line that mimize the distance of all the points to the line.\\


	We consider a line $ y = \alpha x + \beta$, thus we can write the following system

	\begin{align*}
		\beta + 2 \alpha &= 2 \\	
		\beta + 3 \alpha &= 4 \\	
		\beta + 4 \alpha &= 6 \\	
		\beta + 6 \alpha &= 5	
	\end{align*}

\column{0.49\linewidth}

\begin{figure}[ht]
    \centering
    \incfig{leastsquare}
    \label{fig:leastsquare}
\end{figure}

\end{columns}

\end{frame}

\begin{frame}{Useful Resources}{}

	\begin{itemize}
		\item \textbf{Advanced Mathematical Tools for Automatic Control Engineers}. \textit{Alexander S. Poznyak} \href{ https://www.sciencedirect.com/book/9780080446745/advanced-mathematical-tools-for-automatic-control-engineers-deterministic-techniques }{ [Link] }
		\item \textbf{Series of talks about Linear Algebra by 3blue1brown} \href{ https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab }{ [Link] }
		\item \textbf{Video Lectures about Linear Algebra by Dr.Strang} \href{ https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/ }{ [Link] }
	\end{itemize}



\end{frame}


\end{document}
